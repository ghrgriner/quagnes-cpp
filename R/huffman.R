#------------------------------------------------------------------------------
# File: huffman.R
# Author: Ray Griner
# Date: 29-Apr-2024
# Purpose: Estimate how much Huffman compression would reduce the length of
#   strings representing the stock/tableau arrangement in the losing-states
#   (and check-loops) set. We are primarily interested in how well the coding
#   will compress for decks that require high memory use (ie, high
#   `n_losing_states`). We therefore also ran the coding using frequencies only
#   on the csc1.p99 data frame (built on the decks with the top 1% of
#   `n_losing_states`). Lastly of the first 1000 decks, Deck #639 is of special
#   interest, as memory runs out when running this game with the '-t' parameter
#   unspecified.
# Input: tests/up_color_highrun_mem.out
#------------------------------------------------------------------------------

#library(ggplot2)

options(width=120)

#------------------------------------------------------------------------------
# Input file generated by running code at commit: a104d5461da4e96c2937a8c00cae77075979c25e (I think)
# with -y flag for first 1000 simulations.
#------------------------------------------------------------------------------
# Key variables:
#   cum_length - length of string stored in the losing-states set. Should equal
#     number of cards in tableau + 1 for the n stock left.
#   csc0_0 - csc0_63: Suffixes 0-12, 16-28, 32-44, 48-60 represent cards in
#     suits 0-3, respectively. Suffixes 13-14 represent end-of-pile markers for
#     exposed and hidden (face-down) piles in each tableau column. Otherwise 0.
#     This is done for reference and quality control, but we do not expect this
#     to be a very good coding scheme.
#   csc1_0 - csc1_63: Like csc0_[n] variables, except if a card is in-sequence
#     under the same suit, it increments csc_29 instead of its value, and if
#     a card is in-sequence under the same color (but not suit), it increments
#     csc_30 (instead of its value). This will be the primary coding scheme of
#     interest.
#------------------------------------------------------------------------------
df <- read.csv("../tests/up_color_highrun_mem.out")

# Subtract 1 from cum_length/n_losing_states  because the first byte stores
# the n left in stock and will not be compressed using Huffman coding.
# Similarly, do not count this in uncomp.bits.

#df$avg.length = (df$cum_length - 1)/df$n_losing_states
df$avg.length = df$cum_length/df$n_losing_states - 1
df$uncomp.bits = (df$cum_length - df$n_losing_states)*8

head(df)

#hist(df$n_losing_states)
(p99 = quantile(df$n_losing_states, probs=c(.99)))

(df.p99 = subset(df, df$n_losing_states >= p99))
#(df.639 = subset(df, df$rep_id == 639))

# average length in bytes of item stored in set

csc0 <- df[, (names(df) %in% c("rep_id","avg.length","uncomp.bits"))
             | startsWith(names(df), "csc0")]
csc1 <- df[, (names(df) %in% c("rep_id","avg.length","uncomp.bits"))
             | startsWith(names(df), "csc1")]
csc0.p99 = subset(csc0, df$n_losing_states >= p99)
csc0.639 = subset(csc0, df$rep_id == 639)
csc1.p99 = subset(csc1, df$n_losing_states >= p99)
csc1.639 = subset(csc1, df$rep_id == 639)

#------------------------------------------------------------------------------
# Function: set.bits
# Purpose: Assuming a data frame exists that represents a Huffman tree, get the
#  depth of each node and the bits.
# Input parameters:
#   data  - data frame representing a Huffman tree
#   id    - id of node to process
#   depth - depth of current node in tree (root=0)
#   bits  - bits associated with node
# Returns:
#  If no children exist, returns a vector of (id, depth, and bits). Otherwise,
#  returns a data frame that row-binds the returned vectors/matrices returned
#  from calling the function on the left and right nodes after incrementing
#  `depth` and adding "0" or "1" to the bits for the left or right node bits,
#  respectively.
#------------------------------------------------------------------------------
set.bits <- function(data, id, depth, bits) {
  left.id = data[data$id == id,]$left
  right.id = data[data$id == id,]$right
  forhuff <<- data
  if (!is.na(left.id)) {
    ret1 = set.bits(data, left.id, depth+1, paste0(bits,"0"))
    ret2 = set.bits(data, right.id, depth+1, paste0(bits,"1"))
    return (rbind(ret1, ret2))
  }
  else {
    return (data.frame(cbind(id, depth, bits)))
  }
}

#------------------------------------------------------------------------------
# Function: make.huffman
# Purpose: Make a data frame that represents a huffman tree.
#
# Input: A data frame with one row per simulation. Columns "rep_id"
#   and "avg.length" will be dropped, if they exist. All other columns should
#   be either csc_0-csc_63 (in order) or csc_1-csc_63 in order (TODO: enforce
#   this). If the data frame has multiple rows, first the percent for each
#   symbol will be calculated for the given row, and then the mean percent
#   across all rows will be calculated. This mean percent is the frequency
#   used in the Huffman coding. Symbols representing cards (ie, those with
#   id=0-12 + 16*some integer) will be pooled with the other cards of the
#   same rank.
#
# Returns:
#   Data frame with columns:
#     id - id from column name in input dataset (for leaf nodes) or generated
#          as lowest available id >= 64 for others.
#   freq - Frequency used as input to the Huffman coding
#   left - id of left child
#  right - id of right child
# parent - id of parent
#  depth - depth of node in Huffman tree = length of bits
#   bits - bit-encoding of the node.
#-------------------------------------------------------------------------------
make.huffman <- function(data) {
  data <- data[, !names(data) %in% c("rep_id","avg.length","uncomp.bits")]
  data.pct = data/rowSums(data)
  (data.mean = data.frame(cbind(0:63,1,colMeans(data.pct))))
  names(data.mean) <- c("id", "one", "mean")
  data.mean$rank = ifelse((data.mean$id %% 16 >=0)& (data.mean$id %% 16 <= 12),
                           data.mean$id %% 16, data.mean$id)
  data.agg = aggregate(data.mean$mean, by=list(rank=data.mean$rank), FUN=sum)
  names(data.agg)[which(colnames(data.agg)=="x")] = "sum"
  merged = merge(data.mean, data.agg, by="rank")
  # For symbols that represent ordinary cards (ie, not pile markers and not
  # symbols indicating a card that is in-suit or in-color sequence), we use the
  # average frequency across the four suits.
  merged$freq = ifelse((merged$id %% 16 >=0) & (merged$id %% 16 <= 12),
                       merged$sum/4, merged$sum)
  any.found = subset(merged, freq>0)

  forhuff = data.frame(cbind(NA, NA, NA, any.found$id, any.found$freq))
  names(forhuff) <- c("parent","left","right","id","freq")
  #print(forhuff)

  while (1) {
    forhuff <- forhuff[order(forhuff$freq),]
    not.done <- subset(forhuff, is.na(forhuff$parent))
    if (nrow(not.done)<2) break
    else {
      new.id = max(forhuff$id,63)+1
      # make a new row representing the parent
      new.row <- c(NA, not.done[1,"id"], not.done[2,"id"],
                   new.id, sum(not.done[1:2,]$freq))
      # Update the first two rows in not.done with the id of the parent
      not.done[1:2,]$parent = new.id
      # add the new row to the output dataset
      forhuff <- rbind(new.row, not.done[1:2,], not.done[-(1:2),],
                       subset(forhuff, !is.na(forhuff$parent)))
    }
  }

  # Call set.bits on the root of the tree (identified by new.id)
  bits = data.frame(set.bits(forhuff, new.id, 0, ""))
  names(bits) <- c("id","depth","bits")
  bits$depth <- as.numeric(bits$depth)
  output = merge(any.found, bits, by="id")

  # Get the maximum size of a string when `face_up = true`. This is
  #  7 end-of-pile markers for the exposed piles (symbol.13) and every
  #  card except one ace, which we assume for simplicity the excluded
  #  ace is symbol 0
  #---------------------------------------------------------------------
  symbol.13 = subset(output, output$id == 13)
  symbol.other = subset(output, (output$id < 64) & (output$id > 0)
                                & (output$id %% 16 >= 0)
                                & (output$id %% 16 <= 12))
  print(paste0("max.faceup.bytes=",
        max.faceup.bytes = (sum(symbol.other$depth) + 7*symbol.13$depth)/8))

  print(output[order(output$rank),])
  return (output[order(output$rank),])
  #ggplot(data.mean, aes(x="id", y="mean") + geom_bar(stat = "identity"))
}

huff0 <- make.huffman(csc0)
#nocomp <- huff0
#nocomp$depth <- ifelse((huff0$rank>=0 & huff0$rank <= 12), 8, 0)
huff0.p99 <- make.huffman(csc0.p99)
huff0.639 <- make.huffman(csc0.639)

huff1 <- make.huffman(csc1)
#wrong, get the uncompressed size from `cum_length` - `n_losing_states`
#nocomp <- huff1
#nocomp$depth <- ifelse((huff1$rank>=0 & huff1$rank <= 12)
#                      | huff1$rank==29 | huff1$rank==30, 8, 0)
huff1.p99 <- make.huffman(csc1.p99)
huff1.639 <- make.huffman(csc1.639)

#------------------------------------------------------------------------------
# Function: estimate.size
# Parameters:
#   data - input data frame (like csc0, csc1, or some subset thereof)
#  codes - data frame with depth of Huffman codes
# Returns:
#   Data frame with data$rep_id and sum.bits, where sum.bits = total bits
#   needed to encode all the string data using the Huffman coding scheme
#------------------------------------------------------------------------------
estimate.size <- function(data, codes) {
  var.name = substr(deparse(substitute(data)),1,4)
  long.df = reshape(data, direction="long", sep="_", idvar="rep_id",
                    timevar="id", varying=2:65)
  merged = merge(long.df, codes, by="id", all.x=TRUE)
  # either csc0*depth or csc1*depth
  merged$sum.bits = merged[,names(merged) == var.name] * merged$depth
  data.agg = aggregate(merged$sum.bits, by=list(rep_id=merged$rep_id),
                       FUN=sum, na.rm=TRUE)
  names(data.agg) <- c("rep_id", deparse(substitute(codes)))
  print(data.agg)
  return(data.agg)
}

out.huff0     <- estimate.size(csc0.p99, huff0)
out.huff1     <- estimate.size(csc1.p99, huff1)
out.huff0.p99 <- estimate.size(csc0.p99, huff0.p99)
out.huff1.p99 <- estimate.size(csc1.p99, huff1.p99)
out.huff0.639 <- estimate.size(csc0.p99, huff0.639)
out.huff1.639 <- estimate.size(csc1.p99, huff1.639)

start <- csc1[, names(csc1) %in% c("rep_id","avg.length","uncomp.bits")]

merged <- merge(start,  out.huff1,     id="rep_id")
merged <- merge(merged, out.huff0,     id="rep_id")
merged <- merge(merged, out.huff0.p99, id="rep_id")
merged <- merge(merged, out.huff0.639, id="rep_id")
merged <- merge(merged, out.huff1.p99, id="rep_id")
merged <- merge(merged, out.huff1.639, id="rep_id")

merged$huff0.pct     = round(merged$huff0/merged$uncomp.bits,     digits=3)
merged$huff1.pct     = round(merged$huff1/merged$uncomp.bits,     digits=3)
merged$huff0.p99.pct = round(merged$huff0.p99/merged$uncomp.bits, digits=3)
merged$huff1.p99.pct = round(merged$huff1.p99/merged$uncomp.bits, digits=3)
merged$huff0.639.pct = round(merged$huff0.639/merged$uncomp.bits, digits=3)
merged$huff1.639.pct = round(merged$huff1.639/merged$uncomp.bits, digits=3)
print(merged)


